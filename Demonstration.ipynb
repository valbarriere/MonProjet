{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15.2\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's use CRF for a fine grain opinion detection\n",
    "\n",
    "We'll show an exemple using different features, textuals and audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from mesures import F1_token\n",
    "import os\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from sys import platform\n",
    "from numpy import sign\n",
    "\n",
    "# To know if I am on the MAC or on the PC with Linux             \n",
    "CURRENT_OS = platform   \n",
    "if CURRENT_OS == 'darwin':         \n",
    "    INIT_PATH = \"/Users/Valou/\"\n",
    "elif CURRENT_OS == 'linux2':\n",
    "    INIT_PATH = \"/home/valentin/\"\n",
    "\n",
    "path = INIT_PATH + \"Dropbox/TELECOM_PARISTECH/Stage_Lucas/Datasets/Semaine/\"\n",
    "path_model = INIT_PATH + 'Dropbox/TELECOM_PARISTECH/Stage_Lucas/MonProjet/models/' + 'ipython/'\n",
    "path_results = INIT_PATH + '/Dropbox/TELECOM_PARISTECH/Stage_Lucas/MonProjet/results/' + 'ipython/'\n",
    "\n",
    "ALL_LABELS = {'attitude_positive', 'attitude_negative', 'source', 'target'}\n",
    "ALL_FILES = sorted(os.listdir(path+\"all/dump/\")) # nom de tous les fichiers contenus dans path+\"all/dump\" tries dans l'ordre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "### Basic functions (features_text.py)\n",
    "Here we define the function that we will call to make the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def __rules2features(features, sent, i):\n",
    "    \"\"\"\n",
    "    features['inRule'] = 1.0 si la phrase est un pattern detecté par caro\n",
    "    features['inTarget'] = 1.0 si mot i est target du pattern\n",
    "    Cette fonction est foireuse car elle utilise la vérité terrain pour s'entrainer\n",
    "    \"\"\"\n",
    "    \n",
    "    formated_sent = \" \".join([sent[k][0] for k in range(len(sent))])\n",
    "    if formated_sent in PATTERNS:\n",
    "        features['inRule'] = 1.0\n",
    "        target = PATTERNS[formated_sent]\n",
    "        if sent[i][0] in target:\n",
    "            features['inTarget'] = 1.0\n",
    "    return features\n",
    "    \n",
    "    \n",
    "def __features_base(sent, i, nb_neighbours):\n",
    "    \"\"\"\n",
    "    Basic features of each word, including the word and pos-tags of the context\n",
    "    \"\"\"\n",
    "    word = sent[i][0].lower() # literallement le mot sans les maj\n",
    "    postag = sent[i][1][:2] #2 premieres lettres du POS-tag uniquement car decrit plus simplement\n",
    "    features = {\n",
    "        'bias': 1.0, # pourquoi ce bias ?\n",
    "        'word': word,\n",
    "        'postag': postag\n",
    "    }   \n",
    "    \n",
    "    # Number of words that you take into the context\n",
    "    if nb_neighbours == None:\n",
    "        nb_neighbours = 2\n",
    "    \n",
    "    for k in range(1,nb_neighbours+1): # Begin at k = 1\n",
    "        if i > k-1: # If not k-th word of the sentence\n",
    "        \n",
    "            word_neigh_buff = sent[i-k][0].lower()\n",
    "            postag_buff = sent[i-k][1][:2]\n",
    "            features.update({\n",
    "                ('%d:word.lower=' %-k) : word_neigh_buff,\n",
    "                ('%d:postag=' %-k) : postag_buff,\n",
    "            })\n",
    "        else: # If (k-1)-th word = Place In Sentence\n",
    "            features[('P%dIS' %k)] = 1.0\n",
    "        \n",
    "        if i < len(sent) - k: # If not k-th last word of the sentence\n",
    "            word_neigh_buff = sent[i+k][0].lower()\n",
    "            postag_buff = sent[i+k][1][:2]\n",
    "            features.update({\n",
    "                ('%d:word.lower=' %k) : word_neigh_buff,\n",
    "                ('%d:postag=' %k) : postag_buff,\n",
    "            })\n",
    "        else: # If (len(sent) - k)-th word doesn't exist\n",
    "            features[('P%dIS' %-k)] = 1.0\n",
    "\n",
    "    return features            \n",
    "\n",
    "MORPHY_TAG = {'NN': 'n', 'JJ': 'a', 'VB': 'v', 'RB': 'r', 'WR': 'r'}\n",
    "def __swn_scores(features):\n",
    "    \"\"\"\n",
    "    The SentiWordNet scores of each word\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # rappel : MORPHY_TAG = {'NN': 'n', 'JJ': 'a', 'VB': 'v', 'RB': 'r', 'WR': 'r'}\n",
    "        tag_conversion = MORPHY_TAG[features['postag']]\n",
    "    \n",
    "        synset = list(swn.senti_synsets(features['word'], pos=tag_conversion))[0] # variable SWN assez long au niveau du tps\n",
    "        # On choisit le 0         \n",
    "        polarity = [synset.pos_score(), synset.neg_score(), synset.obj_score()] # score SWN (triplet)\n",
    "        features.update({\n",
    "            'sentisynset.pos': polarity[0],\n",
    "            'sentisynset.neg': polarity[1],\n",
    "            'sentisynset.obj': polarity[2]\n",
    "        })\n",
    "#        return polarity\n",
    "    except (KeyError, IndexError):\n",
    "#        return (None,None,None)\n",
    "        pass\n",
    "    \n",
    "    return features\n",
    "    \n",
    "def __phrase_type(sent):\n",
    "    \"\"\"\n",
    "    Return VB if there is a verbe in the sentence\n",
    "    \"\"\"\n",
    "    boolVP = False\n",
    "    for j in range(len(sent)):\n",
    "        if sent[j][1][:2] == 'VB': # 2 premieres lettres du postag du mot j\n",
    "            boolVP = True\n",
    "            return 'VP'\n",
    "    if boolVP is False:\n",
    "        return 'NP'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New features implemented (features_text.py)\n",
    "\n",
    "Used in \n",
    "- <i>Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis</i>,  Wilson et al. \n",
    "- <i>Lexicon-Based Methods for Sentiment Analysis</i>, Taboada et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_LEXICON = INIT_PATH + 'Dropbox/TELECOM_PARISTECH/Corpus-Lexiques/Lexiques/'\n",
    "\n",
    "import pickle\n",
    "SO_CAL_NAME = PATH_LEXICON + 'SO-CAL/SO-CAL_Lexicon.PKL'\n",
    "\n",
    "def return_lexicon(PATH):\n",
    "    f = open(PATH)\n",
    "    var = f.readlines()\n",
    "    f.close()\n",
    "    lex = []\n",
    "    for i in range(len(var)):\n",
    "        lex.append(var[i][:-1])\n",
    "    return lex\n",
    "    \n",
    "# lex [POS+word]=score ; POS : 876 'adv', 2820 'adj', 1539 'noun', 1130 'verb' 217 'int' \n",
    "lex = pickle.load(open(SO_CAL_NAME,'rb'))\n",
    "lex_keys = lex.keys()\n",
    "\n",
    "# the keys are the beginning POS-tag contained in Caro's dumps\n",
    "MORPHY_TAG_LEX = {'NN': 'noun', 'JJ': 'adj', 'VB': 'verb', 'RB': 'adv', 'WR': 'adv'}\n",
    "\n",
    "# On aurait : if MORPHY_TAG_LEX[POS]+word in lex.keys() le mot aurait une valeur d'opinion\n",
    "list_intens = {}\n",
    "for key in lex_keys:\n",
    "    if key[:3] == 'int': list_intens[key[3:]]=lex[key]\n",
    "del list_intens[''] # petit bug a corriger\n",
    "\n",
    "NEGATION_TOKENS = PATH_LEXICON + 'Minqing_Hu/negation-tokens.txt'  \n",
    "list_nega = return_lexicon(NEGATION_TOKENS)\n",
    "list_nega_0 = [\"n't\",'not','no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __negation(sent, i, context):\n",
    "    \"\"\"\n",
    "    If there is a negation at least context word before the word\n",
    "    ---------->WORK<----------\n",
    "    \"\"\"\n",
    "    bool_neg = False\n",
    "    for k in range(1,context+1): # Begin at k = 1\n",
    "        if i > k-1 and ( sent[i-k][0].lower() in list_nega_0) : # If not k-th word of the sentence\n",
    "            bool_neg = True\n",
    "    \n",
    "    return bool_neg\n",
    "    \n",
    "def __newI(sent,i):\n",
    "    \"\"\"\n",
    "    Return True if there is a \"and\" (pos = CC) then a I\n",
    "    \n",
    "    ---------->DON'T WORK<----------\n",
    "    \"\"\"\n",
    "    if i > 0 and sent[i-1][1][:2] == \"CC\" and sent[i][0] ==\"i\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "################################ New features ############################\n",
    "def __adj(sent,i):\n",
    "    \"\"\"\n",
    "    Return True if the word is a noun preceded by an adj\n",
    "    \"\"\"\n",
    "    if i > 0 and sent[i-1][1][:2] == \"JJ\" and sent[i][1][:2] ==\"NN\":\n",
    "        return True\n",
    "    else:\n",
    "        return False    \n",
    "\n",
    "def __adv(sent,i):\n",
    "    \"\"\"\n",
    "    Return True if the preceding word is an adverb other than not\n",
    "    \"\"\"\n",
    "    if i > 0 and sent[i-1][1][:2] == \"RB\" and sent[i-1][0].lower() !=\"not\":\n",
    "        return True\n",
    "    else:\n",
    "        return False    \n",
    "\n",
    "\n",
    "######## Features de Lexicon-based approach #############\n",
    "\n",
    "def __intens(sent,i):\n",
    "    \"\"\"\n",
    "    Return True if the preceding word is an intensifier\n",
    "    \"\"\"\n",
    "    if i > 0 and sent[i-1][0].lower() in list_intens:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def __intens_is(sent,i):\n",
    "    \"\"\"\n",
    "    Return True if the word itself is an intensifier\n",
    "    \"\"\"\n",
    "    if sent[i][0].lower() in list_intens:\n",
    "        return True\n",
    "    else:\n",
    "        return False \n",
    "\n",
    "\n",
    "def __SO_value(sent,i):\n",
    "    \"\"\"\n",
    "    Return the SO value of the word if it has one, False either\n",
    "    \"\"\"\n",
    "    word = sent[i][0].lower() # literallement le mot sans les maj\n",
    "    postag = sent[i][1][:2] #2 premieres lettres du POS-tag uniquement car decrit plus simplement\n",
    "    \n",
    "    if postag+word in lex_keys:\n",
    "        return lex[postag+word]\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2feature function (features_text.py)\n",
    "Function that will transform the different words into the features used by the CRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the new features in the list of you add it in params\n",
    "LIST_FEATURES = ['nb_neighbours','context_negation','rules_synt', 'newI', \\\n",
    "'rules_synt', 'swn_scores','swn_pos', 'swn_neg', 'swn_obj', 'inverse_score', \\\n",
    "'adj', 'adv','intens','intens_is','SO_value','SO_intensifier','SO_negation']\n",
    "\n",
    "def __word2features(sent, i, params):\n",
    "    u\"\"\"Features lexicaux et syntaxiques.\n",
    "    nb_neighbours est la taille du contexte que l'on prend en nombre de mots\n",
    "    ---->Si l'on veut ajouter une feature, il faut la mettre dans params et ensuite \n",
    "    l'ajouter dans la liste LIST_FEATURES pour la prendre en compte si elle y est   \n",
    "    \"\"\"      \n",
    "    boolean = {}\n",
    "    \n",
    "    for k in LIST_FEATURES:\n",
    "        if k in params:\n",
    "            boolean[k] = params[k]\n",
    "        else:\n",
    "            boolean[k] = 0\n",
    "      \n",
    "    # Basic features of each word : those ones are always there by default\n",
    "    features = __features_base(sent, i, boolean['nb_neighbours'])\n",
    "    \n",
    "    # si ya VB ds la phrase VP, sinon NP\n",
    "    features['phrase_type'] = __phrase_type(sent) \n",
    "    \n",
    "    ### SWN score ###\n",
    "    if boolean['swn_scores'] != 0:\n",
    "        features = __swn_scores(features)\n",
    "        \n",
    "    if 'sentisynset.pos' in features.keys(): \n",
    "        bool_senti = True\n",
    "    else: \n",
    "        bool_senti = False \n",
    "\n",
    "    ### More SWN score ####\n",
    "    if boolean['swn_pos'] != 0 and bool_senti:\n",
    "        features['swn_pos_score_bis'] = features['sentisynset.pos']    \n",
    "    if boolean['swn_neg'] != 0 and bool_senti:\n",
    "        features['swn_pos_score_bis'] = features['sentisynset.neg']            \n",
    "    if boolean['swn_obj'] != 0 and bool_senti:\n",
    "        features['swn_obj_score_bis'] = features['sentisynset.obj']      \n",
    "    \n",
    "    if boolean['context_negation'] != 0:\n",
    "        features['negation'] = __negation(sent, i, boolean['context_negation'])\n",
    "    \n",
    "    # True if there is a 'I' after a 'and'\n",
    "    if boolean['newI'] == True:\n",
    "        features['newI'] = __newI(sent,i)\n",
    "    \n",
    "    # rules_syntaxic\n",
    "    if boolean['rules_synt'] == True:\n",
    "        features = __rules2features(features, sent, i)\n",
    "        \n",
    "    # If negation before, inverse the pos and neg score\n",
    "    if boolean['inverse_score'] == True and features['negation'] == True and bool_senti:\n",
    "        buff = features['sentisynset.pos']        \n",
    "        features['sentisynset.pos'] = features['sentisynset.neg']\n",
    "        features['sentisynset.neg'] = buff\n",
    "    \n",
    "    if boolean['adj'] == True:\n",
    "        features['adj'] = __adj(sent,i)\n",
    "        \n",
    "    if boolean['adv'] == True:\n",
    "        features['adv'] = __adv(sent,i)\n",
    "\n",
    "    if boolean['intens'] == True:\n",
    "        features['intens'] = __intens(sent,i)\n",
    "        \n",
    "    if boolean['intens_is'] == True:\n",
    "        features['intens_is'] = __intens_is(sent,i)        \n",
    "    \n",
    "        \n",
    "    if boolean['SO_value'] == True:\n",
    "        SO_value = __SO_value(sent,i)\n",
    "        # If there is a value\n",
    "        if SO_value:\n",
    "            # if the word is before intensifier\n",
    "            if boolean['SO_intensifier']:\n",
    "                if __intens(sent,i) and SO_value:\n",
    "                    SO_value = (1+list_intens[sent[i].lower()])*SO_value\n",
    "                #if __intens_is(sent,i) and __SO_value(sent,i+1):\n",
    "                #    SO_val = (1+list_intens[sent[i].lower()])*__SO_value(sent,i+1)\n",
    "            \n",
    "            if boolean['SO_negation']:        \n",
    "                if __negation(sent, i, boolean['SO_negation']):\n",
    "                    SO_value = SO_value - 4*sign(SO_value)\n",
    "            \n",
    "            features['SO_value'] = SO_value\n",
    "    \n",
    "    \n",
    "    return features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features to crfsuite (exctraction.py)\n",
    "\n",
    "Prepare the text features from the functions above and the audio features from the dumps (for now, after we'll use the pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MULTILABEL = ('B-evaluation', 'B-affect', 'I-evaluation', 'I-affect',\n",
    "              'B-source', 'I-source', 'B-target', 'I-target')\n",
    "              \n",
    "HIERARCHY = {'I-attitude_positive': 1, 'B-attitude_positive': 2, 'I-attitude_negative': 3, 'B-attitude_negative': 4, 'I-source': 5, 'B-source': 6,\n",
    "             'I-target': 7, 'B-target': 8, 'O': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __merge_dicts(*dict_args):\n",
    "    u\"\"\"Fusionne n'importe quel nombre de dict.\"\"\"\n",
    "    z = {}\n",
    "    for y in dict_args:\n",
    "        z.update(y)\n",
    "    return z\n",
    "\n",
    "\n",
    "def __audio2features(audio, i):\n",
    "    \"\"\"\n",
    "    STRING --> DICTIONNAIRE pour un seul mot (le i)\n",
    "    Permet de mettre le dictionnaire qui n'etait qu'une string sous vrai forme de dictionnaire\n",
    "    i est le numero du mot, on obtient donc toutes les features audio pour un seul mot\n",
    "    \"\"\"\n",
    "\n",
    "    dict_pitch = eval(audio[i])\n",
    "    # result au lieu de dict : refaire dico en enlevant les None\n",
    "    result_pitch = {}\n",
    "    for k, v in dict_pitch.items():\n",
    "        if dict_pitch[k] != None:\n",
    "            result_pitch[k] = v\n",
    "    return result_pitch\n",
    "\n",
    "\n",
    "def __sent2features(sent, audio, mfcc, params):\n",
    "    u\"\"\"Choisir les types de features utilisés ici.\n",
    "    Avec opt, on ne garde qu'audio ou texte si on veut faire des test séparement.\n",
    "    Il n'y a qu'a fusionner les dict voulus pour chaque mot\n",
    "    \n",
    "    sent, audio et mfcc sont d'une seule phrase --> len(sent) est le nbr de mot i le numero du mot\n",
    "    \"\"\"\n",
    "    opt = params['opt']\n",
    "    if opt == 'MULTI':\n",
    "        return [__merge_dicts(__word2features(sent, i, params),\n",
    "                          __audio2features(audio,i)) for i in range(len(sent))] \n",
    "    elif opt == 'AUDIO':             \n",
    "        return [__merge_dicts(__audio2features(audio,i)) for i in range(len(sent))] \n",
    "    else: # then it's just text\n",
    "        return [__merge_dicts(__word2features(sent, i, params)) for i in range(len(sent))] \n",
    "\n",
    "def __sent2label(sent, label):\n",
    "    \"\"\"\n",
    "    Return a list with the labels of eahc word in 1 sentence\n",
    "    \"\"\"\n",
    "    return [__decision(str_labels, label) for token, postag,\n",
    "            str_labels in sent]\n",
    "\n",
    "\n",
    "def __sent2tokens(sent):\n",
    "    \"\"\"List of the words from a sentence\n",
    "    NOT USED    \n",
    "    \"\"\"\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "\n",
    "def __decision(str_labels, label):\n",
    "    \"\"\"\n",
    "    label peut etre attitude, source ou target : pour un entrainement séparé \n",
    "    mais qui est moins efficace que s'il est bien fait ensemble\n",
    "    \"\"\"\n",
    "    list_labels = str_labels.split(\";\") # S'il y a plusieurs labels par mot\n",
    "    if label == 'BIO': # garder toutes les annotations \n",
    "        \n",
    "#        rappel : HIERARCHY = {'I-attitude_positive': 1, 'B-attitude_positive': 2, 'I-attitude_negative': 3, 'B-attitude_negative': 4, 'I-source': 5, 'B-source': 6,\n",
    "#             'I-target': 7, 'B-target': 8, 'O': 9}\n",
    "             \n",
    "        list_nb = [HIERARCHY[lab] for lab in list_labels] # donne un \"rang\" aux differents labels du mot\n",
    "        return \"\".join([k for k, v in HIERARCHY.items() # regarde les rangs des differents labels\n",
    "                        if v == np.min(list_nb)]) # label qui a le \"rang\" le plus eleve (nb le + petit) gagne\n",
    " \n",
    "    else: # Si pas BIO, c'est attitude par exemple, on les entraine separement !! (d'abord attitude ou source ou ?)\n",
    "        \n",
    "        if label.__class__ == list: # plusieurs labels de sortie --> ex : ['attitde_positive' ,'attitude_negative']\n",
    "            for k in range(len(label)):            \n",
    "                if \"I-\"+label[k] in list_labels:\n",
    "                    return \"I-\"+label[k]\n",
    "                elif \"B-\"+label[k] in list_labels:\n",
    "                    return \"B-\"+label[k]    \n",
    "                    \n",
    "            return \"O\"# if no label\n",
    "        else: # only one label\n",
    "            if \"I-\"+label in list_labels:\n",
    "                return \"I-\"+label\n",
    "            elif \"B-\"+label in list_labels:\n",
    "                return \"B-\"+label\n",
    "            else:\n",
    "                return \"O\"\n",
    "\n",
    "def text_sents(path):\n",
    "    u\"\"\"Traite le texte. \n",
    "    Pas utilisé, on le fait avec nltk.corpus.conll2002.iob_sents(path_text)\n",
    "    \"\"\"\n",
    "    f = open(path, 'Ur')\n",
    "    sents = f.read().split('\\n\\n\\n') # Phrase \n",
    "    sents_list = []\n",
    "    for sent in sents:\n",
    "        words = sent.split('\\n') # Mots \n",
    "        words_list = []\n",
    "        for word in words:\n",
    "            features = tuple(word.split('\\t'))[:2]\n",
    "            words_list.append(features)\n",
    "        sents_list.append(words_list)\n",
    "    return sents_list\n",
    "    \n",
    "\n",
    "def audio_sents(path):\n",
    "    u\"\"\"Traite l'audio.\n",
    "    Va cherche les dumps et les met dans des variables\n",
    "    /Datasets/Semaine/all/+ dump_audio/ ou dump_mfcc/    \n",
    "    \"\"\"\n",
    "    f = open(path, 'Ur')\n",
    "    sents = f.read().split('\\n\\n\\n') # sent[0] 1ere phrase\n",
    "    sents_list = []\n",
    "    for sent in sents: # pour chaque phrase\n",
    "        words = sent.split('\\n') # separation par mot words[0] 1er mot\n",
    "        words_list = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                features = word.split('\\t')[1] # Separe le mot ex 'HI' des features (u'moy_loc_B1': -0.059, u'moy_loc_B2': -0.199)\n",
    "                if features == 'None':\n",
    "                    features = \"{}\"\n",
    "                words_list.append(features)\n",
    "                m = re.findall(r\"\\>|\\<|'|GONNA|WANNA\", word.split('\\t')[0]) # trouve les gonna/wanna/'/ qui font 2 mots avc POSTAG \n",
    "                #  --> don't = do not, pour l'audio on met les memes features audio pour les 2 mots\n",
    "                for k in range(len(m)):\n",
    "                    words_list.append(features)\n",
    "            except IndexError:\n",
    "                #  print('END OF FILE %s' % path.split('.')[0][-3:]) #fin du fichier, donne le nom de la session en +\n",
    "                break\n",
    "        sents_list.append(words_list)\n",
    "    return sents_list  \n",
    "\n",
    "\n",
    "def extract2CRFsuite(path_text, path_audio, path_mfcc, label='BIO', params = None):\n",
    "    u\"\"\"PLUS IMPORTANTE.\n",
    "    \n",
    "    Extrait features et label pour une session\n",
    "    à partir d'un dossier contenant les dump au format Conll\n",
    "    \"\"\"\n",
    "    # Just to charge the good ones : \n",
    "    text = None\n",
    "    audio = None\n",
    "    mfcc = None\n",
    "    opt = params['opt']\n",
    "    text = nltk.corpus.conll2002.iob_sents(path_text) # text[phrase][mot] = (mot, genre NN, BIO-attitude)\n",
    "    \n",
    "    # Labels first sice we need the text   \n",
    "    y = [__sent2label(s, label) for s in text]    \n",
    "    \n",
    "    # Then the variables\n",
    "    if opt == 'TEXT':\n",
    "        audio = [None]*len(text)\n",
    "        mfcc =  [None]*len(text)\n",
    "    elif opt == 'AUDIO':\n",
    "        audio = audio_sents(path_audio) # audio[phrase][mot] = string avec les valeurs (string d'un dictionnaire)\n",
    "        #  par ex : \"{u'moy_loc_B1': -0.059879489425627798, u'moy_loc_B2': -0.19947861555547755, u'moy_loc_F1': 0.026468}\"\n",
    "        mfcc = audio_sents(path_mfcc)\n",
    "        text =  [None]*len(audio)\n",
    "    elif opt == 'MULTI':\n",
    "        audio = audio_sents(path_audio) # audio[phrase][mot] = string avec les valeurs (string d'un dictionnaire)\n",
    "        #  par ex : \"{u'moy_loc_B1': -0.059879489425627798, u'moy_loc_B2': -0.19947861555547755, u'moy_loc_F1': 0.026468}\"\n",
    "        mfcc = audio_sents(path_mfcc)\n",
    "        \n",
    "    X = [__sent2features(s, t, u, params) for (s, t, u) in zip(text, audio, mfcc)] # on prend phrase par phrase\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training and testing (main_val.py)\n",
    "\n",
    "### Parameters\n",
    "\n",
    "#### CRF\n",
    "Set training parameters. We will use L-BFGS training algorithm (it is default) without Elastic Net (L1 + L2) regularization but just L2 norm regularization since we don't have a lot of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['c1'] = 0\n",
    "params['c2'] = 1e-2\n",
    "params['max_it'] = 50\n",
    "\n",
    "params['c2'] = 1e-3\n",
    "params['c1'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features\n",
    "We choose to apply the algo only on the TEXT, on the AUDIO, or on BOTH with the 'opt' parameter.\n",
    "\n",
    "We can choose which textual features we are going to use (permitt to test a lot of different features and just select the best ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params['opt'] = 'TEXT' \n",
    "params['context_negation'] = 2\n",
    "params['nb_neighbours'] = 2\n",
    "params['newI'] = False\n",
    "params['swn_scores'] = True\n",
    "\n",
    "params['rules_synt'] = False\n",
    "params['swn_score'] = True\n",
    "params['inverse_score'] = False\n",
    "\n",
    "params['swn_pos'] = True\n",
    "params['swn_neg'] = True\n",
    "\n",
    "params['adj'] = True\n",
    "params['adv'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kind of labels \n",
    "\n",
    "We can select the labels we want to have on the different words and the label we wanna detect at the end. \n",
    "\n",
    "If we want to keep the difference between positive and negative attitude and just detect the attitude positive : \n",
    "--> label_att = ['attitude_negative','attitude_positive'] ; label_select = 'attitude_positive' \n",
    "\n",
    "If we want to just detect the positive attitude and not the negative one\n",
    "--> label_att = 'attitude_positive' ; label_select = 'attitude_positive' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_att = ['attitude_negative','attitude_positive'] ; label_select = 'attitude_positive' ; valence = True\n",
    "\n",
    "#label_att = 'attitude_positive' ; label_select = 'attitude_positive' ; valence = True\n",
    "#label_att = 'attitude' ; label_select = None ; valence = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the features of a word look like : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-1:postag=': u'VB',\n",
       " '-1:word.lower=': u'red',\n",
       " '-2:postag=': u'VB',\n",
       " '-2:word.lower=': u\"'ve\",\n",
       " '1:postag=': u'CC',\n",
       " '1:word.lower=': u'and',\n",
       " '2:postag=': u'DT',\n",
       " '2:word.lower=': u'a',\n",
       " 'adj': False,\n",
       " 'adv': False,\n",
       " 'bias': 1.0,\n",
       " 'negation': False,\n",
       " 'phrase_type': 'VP',\n",
       " 'postag': u'NN',\n",
       " 'sentisynset.neg': 0.0,\n",
       " 'sentisynset.obj': 1.0,\n",
       " 'sentisynset.pos': 0.0,\n",
       " 'swn_pos_score_bis': 0.0,\n",
       " 'word': u'hair'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = ALL_FILES[0]\n",
    "X_test, y_test = extract2CRFsuite(path+\"all/dump\"+valence*\"_attitudeposneg_only\"+\"/\"+filename,\n",
    "                                path+\"all/dump_audio/\"+filename,\n",
    "                                path+\"all/dump_mfcc/\"+filename,\n",
    "                                label_att, params)\n",
    "X_test[3][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dump_resultats(precision, recall, F1, filename):\n",
    "    u\"\"\"Dump the results.\"\"\"\n",
    "    f = open(filename, 'w')\n",
    "    f.write(\"Session\\t\\tPrecision\\tRecall\\tF1\\n\")\n",
    "    \n",
    "    session = 'overall' # Every sessions on the 1st line, then\n",
    "    f.write(\"%s\\t\\t%s\\t\\t%s\\t\\t%.2f\\n\" % (session, precision[session], recall[session], F1))\n",
    "    alz = precision.copy()\n",
    "    del alz['overall']\n",
    "    for session in alz.keys():\n",
    "        f.write(\"%s\\t%s\\t\\t%s\\n\" % (session, precision[session], recall[session]))\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "def cvloo(label, path_results, params, label_select=None, LOOP_TEST=False, valence = False):\n",
    "    u\"\"\"Compute the Cross-validation for the given label.\n",
    "    valence is True if we wanna distinguish the positive and negative attitudes    \n",
    "    \"\"\"\n",
    "    if label_select is None:\n",
    "        label_select = label\n",
    "    opt = params['opt']\n",
    "    \n",
    "    truepos_o, falsepos_o, falseneg_o = (0, 0, 0)    \n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    \n",
    "    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "        \n",
    "    \n",
    "    for i in range(len(ALL_FILES)):\n",
    "        filename = ALL_FILES[i]\n",
    "        X, y = extract2CRFsuite(path+\"all/dump\"+valence*\"_attitudeposneg_only\"+\"/\"+filename,\n",
    "                                path+\"all/dump_audio/\"+filename,\n",
    "                                path+\"all/dump_mfcc/\"+filename,\n",
    "                                label, params)\n",
    "        for x_seq, y_seq in zip(X, y):\n",
    "            trainer.append(x_seq, y_seq, i)\n",
    "        \n",
    "    trainer.set_params({\n",
    "        'c1': params['c1'],   # coefficient for L1 penalty\n",
    "        'c2': params['c2'],  # coefficient for L2 penalty\n",
    "        'max_iterations': params['max_it'],  # stop earlier\n",
    "\n",
    "        # include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': False,\n",
    "    })\n",
    "    #print(\"Beginning of the training\")\n",
    "    for i in range(len(ALL_FILES)):\n",
    "    #for i in range(1):\n",
    "        \n",
    "        filename = ALL_FILES[i]\n",
    "        filename_model = filename.split('.')[0] # to threw away the extension\n",
    "        \n",
    "        # Training \n",
    "        trainer.train(path_model+'model_%s_' %opt + filename_model, i)\n",
    "\n",
    "        # Testing\n",
    "        X_test, y_test = extract2CRFsuite(path+\"all/dump\"+valence*\"_attitudeposneg_only\"+\"/\"+filename,\n",
    "                                path+\"all/dump_audio/\"+filename,\n",
    "                                path+\"all/dump_mfcc/\"+filename,\n",
    "                                label, params)\n",
    "        tagger = pycrfsuite.Tagger(verbose=False)\n",
    "        tagger.open(path_model + 'model_%s_' %opt + filename_model)\n",
    "        \n",
    "        truepos, falsepos, falseneg = (0, 0, 0)\n",
    "        for sent, corr_labels in zip(X_test, y_test):\n",
    "            pred_labels = tagger.tag(sent)\n",
    "            trueposAdd, falseposAdd, falsenegAdd = \\\n",
    "                F1_token(\n",
    "                    pred_labels,\n",
    "                    corr_labels,\n",
    "                    label_select)\n",
    "            truepos += trueposAdd\n",
    "            falsepos += falseposAdd\n",
    "            falseneg += falsenegAdd\n",
    "        \n",
    "        precision[filename] = \"%.2f\" % (truepos/(truepos+falsepos+0.01) * 100)\n",
    "        recall[filename] = \"%.2f\" % (truepos/(truepos+falseneg+0.01) * 100)            \n",
    "        truepos_o += truepos\n",
    "        falsepos_o += falsepos\n",
    "        falseneg_o += falseneg\n",
    "        \n",
    "    precision['overall'] = \"%.2f\" % (truepos_o/(truepos_o+falsepos_o+0.01) * 100)\n",
    "    recall['overall'] = \"%.2f\" % (truepos_o/(truepos_o+falseneg_o+0.01) * 100)\n",
    "    F1 = 2*float(precision['overall'])*float(recall['overall'])/(float(precision['overall'])+float(recall['overall'])+1e-5)\n",
    "\n",
    "    # If there is pos and neg differentiation for the attitudes\n",
    "    if valence == True and label.__class__ == list: label = 'attitud_posneg'\n",
    "\n",
    "    ext = '.txt'\n",
    "    dump_resultats(precision, recall, F1, path_results + 'results_CVLOO_%s_' %(opt) +label+\"_\"+label_select+ext)\n",
    "    if LOOP_TEST: # if loop test dump the ALL the results in 1 file : useful for hyperparams test\n",
    "        dump_resultats_total(precision, recall, F1, path_results + 'results_total_%s_' %(opt) +label+\"_\"+label_select+ext, params)\n",
    "    return_sent = 'Precision : %s, Recall : %s, F1 : %.2f' %(precision['overall'], recall['overall'], F1)\n",
    "    return return_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the label None : \n",
      "Precision : 57.89, Recall : 40.67, F1 : 47.78\n"
     ]
    }
   ],
   "source": [
    "params['c2'] = 1e-3\n",
    "params['c1'] = 0\n",
    "params['opt'] = 'TEXT' \n",
    "params['context_negation'] = 2\n",
    "params['nb_neighbours'] = 2\n",
    "params['rules_synt'] = False\n",
    "params['newI'] = False\n",
    "params['swn_score'] = True\n",
    "\n",
    "# inverse the swn score if there is a negation\n",
    "params['inverse_score'] = False\n",
    "# put several time the swn_pos/neg score in the features\n",
    "params['swn_pos'] = False\n",
    "params['swn_neg'] = False\n",
    "params['swn_obj'] = False\n",
    "\n",
    "# Detect adverb, adjective or intensifier\n",
    "params['adv'] = False\n",
    "params['adj'] = False\n",
    "params['intens'] = False\n",
    "params['intens_is'] = False\n",
    "\n",
    "\n",
    "# With the Semontic Opinion score : other lexique\n",
    "params['SO_value'] = False \n",
    "params['SO_intensifier'] = False\n",
    "params['SO_negation'] = False\n",
    "\n",
    "\n",
    "#label_att = ['attitude_negative','attitude_positive'] ; label_select = 'attitude_positive' ; valence = True\n",
    "#label_att = 'attitude_positive' ; label_select = 'attitude_positive' ; valence = True\n",
    "label_att = 'attitude' ; label_select = None ; valence = False\n",
    "\n",
    "print 'For the label %s : ' %(label_select)\n",
    "print cvloo(label_att, path_results, params, label_select = label_select, valence=valence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire un test avec d'autres labels : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the label None : \n",
      "Precision : 57.17, Recall : 39.26, F1 : 46.55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "label_att = 'attitude' ; label_select = None ; valence = False\n",
    "print 'For the label %s : ' %(label_select)\n",
    "print cvloo(label_att, path_results, params, label_select = label_select, valence=valence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the classifier learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transitions between words regarding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "I-attitude_negative -> I-attitude_negative 5.978109\n",
      "B-attitude_positive -> I-attitude_positive 5.512919\n",
      "I-attitude_positive -> I-attitude_positive 5.480694\n",
      "O      -> O       4.986429\n",
      "B-attitude_negative -> I-attitude_negative 4.896438\n",
      "O      -> B-attitude_positive 0.295560\n",
      "O      -> B-attitude_negative 0.143329\n",
      "I-attitude_positive -> B-attitude_positive 0.059109\n",
      "I-attitude_negative -> B-attitude_negative -0.162060\n",
      "I-attitude_positive -> B-attitude_negative -0.429104\n",
      "\n",
      "Top unlikely transitions:\n",
      "B-attitude_positive -> O       -1.434754\n",
      "I-attitude_positive -> O       -1.827552\n",
      "I-attitude_negative -> O       -2.410462\n",
      "O      -> I-attitude_negative -3.331075\n",
      "O      -> I-attitude_positive -3.723805\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "filename_model = ALL_FILES[0].split('.')[0] # to threw away the extension\n",
    "tagger = pycrfsuite.Tagger(verbose=False)\n",
    "tagger.open(path_model + 'model_%s_' %params['opt'] + filename_model)\n",
    "\n",
    "# What we use to have the weights : tagger is really important\n",
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the dict of weights in counter type and sort the list \n",
    "# info.transitions[('B-LOC','B-LOC')] gives the weights between B-LOC then B-LOC\n",
    "\n",
    "# gives the 10 firsts\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(10))\n",
    "\n",
    "# The 5 lasts\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most/Less weighted features regarding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "2.457445 O      P1IS\n",
      "2.362237 O      P-1IS\n",
      "1.985765 B-attitude_positive P1IS\n",
      "1.880477 B-attitude_negative P1IS\n",
      "1.051011 O      P-2IS\n",
      "0.997364 O      postag:NN\n",
      "0.872209 I-attitude_negative P-2IS\n",
      "0.833562 O      word:em\n",
      "0.816481 I-attitude_positive P-2IS\n",
      "0.800147 I-attitude_positive -1:postag=:NN\n",
      "0.785202 O      word:yeah\n",
      "0.749577 I-attitude_negative -1:postag=:NN\n",
      "0.732676 O      bias\n",
      "0.708124 O      phrase_type:NP\n",
      "0.682290 I-attitude_positive sentisynset.pos\n",
      "0.681614 O      postag:CC\n",
      "0.626068 O      word:and\n",
      "0.602122 I-attitude_negative -1:word.lower=:hm\n",
      "0.601450 O      2:word.lower=:>\n",
      "0.601450 O      word:<\n",
      "\n",
      "Top negative:\n",
      "-0.489057 I-attitude_positive -2:postag=:PO\n",
      "-0.496427 I-attitude_positive word:and\n",
      "-0.500166 I-attitude_positive word:have\n",
      "-0.502683 O      word:you\n",
      "-0.502894 O      postag:VB\n",
      "-0.503884 I-attitude_negative postag:CC\n",
      "-0.527920 B-attitude_positive phrase_type:NP\n",
      "-0.544279 O      word:god\n",
      "-0.580185 B-attitude_negative phrase_type:NP\n",
      "-0.594465 O      word:excellent\n",
      "-0.626841 O      1:word.lower=:and\n",
      "-0.655257 B-attitude_positive -1:postag=:NN\n",
      "-0.656450 I-attitude_negative postag:NN\n",
      "-0.716473 O      -1:postag=:IN\n",
      "-0.829279 I-attitude_positive postag:CC\n",
      "-0.976098 O      1:postag=:NN\n",
      "-1.178975 B-attitude_negative P-2IS\n",
      "-1.227561 B-attitude_negative P-1IS\n",
      "-1.433159 B-attitude_positive P-1IS\n",
      "-1.560727 B-attitude_positive P-2IS\n"
     ]
    }
   ],
   "source": [
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the CRF regard a lot if the word is surrounded by a word at the left and at the right"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
